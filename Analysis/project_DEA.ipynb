{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d755b7",
   "metadata": {},
   "source": [
    "# Plan.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7abcf",
   "metadata": {},
   "source": [
    "•⁠  ⁠The main goal is to create an energy prediction model of prosumers to reduce energy imbalance costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01892d8",
   "metadata": {},
   "source": [
    "# Analyze.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b984f",
   "metadata": {},
   "source": [
    "-- Asks for my analysis:\n",
    "\n",
    "•⁠  ⁠⁠What are the most important reasons affecting production or consumption ?\n",
    "\n",
    "•⁠  ⁠What product type consume the most energy?\n",
    "\n",
    "•⁠  ⁠What product type production most energy ?\n",
    "\n",
    "•⁠  ⁠⁠What are the perfect conditions for each product in terms of the most important factors affecting it?\n",
    "\n",
    "•⁠  ⁠⁠What the appropriate budget to cover the requirements of each business product ?\n",
    "\n",
    "•⁠  When is product consumption above the limit or outlier?\n",
    "\n",
    "•⁠  When consumption or production highest for each product ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly as plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "from scipy.stats import probplot\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed5a60",
   "metadata": {},
   "source": [
    "**- Prepare dataset for analysis..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c78a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train=pd.read_csv('/content/drive/MyDrive/predict-energy-behavior-of-prosumers/client.csv')\n",
    "electricity_train=pd.read_csv('/content/drive/MyDrive/predict-energy-behavior-of-prosumers/electricity_prices.csv')\n",
    "hist_weather_train=pd.read_csv('/content/drive/MyDrive/predict-energy-behavior-of-prosumers/historical_weather.csv')\n",
    "gas_price_train=pd.read_csv('/content/drive/MyDrive/predict-energy-behavior-of-prosumers/gas_prices.csv')\n",
    "train=pd.read_csv('/content/drive/MyDrive/predict-energy-behavior-of-prosumers/train.csv')\n",
    "locations=pd.read_csv('/content/drive/MyDrive/archive-4/county_lon_lats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info() #Convert type to datetime column ( Object --> datetime )\n",
    "             #Remove row_id columns\n",
    "             #Seperate Datetime into (month,year,day,hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc565f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum() # Remove all rows contain null value in target field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train.info()#Convert type to datetime column ( Object --> datetime )\n",
    "                   #Seperate Datetime into (month,year,day,hour)\n",
    "                   #Subtract 1 from data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train.isnull().sum() #Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371747c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_train.info() #Convert type to datetime column ( Object --> datetime )\n",
    "                         #Seperate Datetime into (hour)\n",
    "                         #Subtract 1 from data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a503fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_weather_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_weather_train.info()#Convert type to datetime column ( Object --> datetime )\n",
    "                         #Seperate Datetime into (hour)\n",
    "                         #Subtract 1 from data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e999aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_weather_train.isnull().sum() #Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83017b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_price_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26308847",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_price_train.info()#Subtract 1 from data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_price_train.isnull().sum() #Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48be5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations #I need this data for merge with weather data because I need knowing county for each lognitude and latitude .\n",
    "          # Drop 'Unnamed: 0' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d481c8",
   "metadata": {},
   "source": [
    "**- Build functions to solve all problems in datasets the merge togther .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_client(client):\n",
    "    client['data_block_id']-=2\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebe365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_gas(gas):\n",
    "    gas['data_block_id']-=1\n",
    "    return gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_electricity(electricity):\n",
    "    electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n",
    "    electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n",
    "    electricity['hour'] = electricity['datetime'].dt.hour\n",
    "    electricity['data_block_id']-=1\n",
    "    return electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_weather(weather,locations):\n",
    "    hist_weather=weather[['temperature','dewpoint','cloudcover_total', 'cloudcover_low','latitude', 'longitude',\n",
    "       'cloudcover_mid', 'cloudcover_high','direct_solar_radiation','snowfall','datetime','data_block_id']]\n",
    "    locations = locations.drop('Unnamed: 0', axis= 1)\n",
    "    hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n",
    "    hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])\n",
    "    hist_weather.dropna(axis= 0, inplace= True)\n",
    "    hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n",
    "    hist_weather['county'] = hist_weather['county'].astype('int64')\n",
    "    hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n",
    "    hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime','data_block_id'], axis= 1).columns)].mean().reset_index()\n",
    "    hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n",
    "    hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index()\n",
    "    hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "    hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n",
    "    hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n",
    "    hist_weather_datetime.drop_duplicates(inplace=True)\n",
    "    hist_weather_datetime_county.drop_duplicates(inplace=True)\n",
    "    hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n",
    "    hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n",
    "    return(hist_weather_datetime_county,hist_weather_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4928363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_train(train):\n",
    "  train= train[train['target'].notnull()]\n",
    "  train['datetime'] = pd.to_datetime(train['datetime'], utc=True)\n",
    "  train['year'] = train['datetime'].dt.year\n",
    "  train['month'] = train['datetime'].dt.month\n",
    "  train['day'] = train['datetime'].dt.day\n",
    "  train['hour'] = train['datetime'].dt.hour\n",
    "  return (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_merge(data,client,hist_weather_datetime_county,hist_weather_datetime,electricity,gas):\n",
    "    data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "    data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "    data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n",
    "    data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n",
    "    data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],\n",
    "                     suffixes= ('_hist_mean','_hist_mean_by_county'))\n",
    "    data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda row: row.ffill().bfill()).reset_index()\n",
    "    data.drop(['row_id','index', 'data_block_id'],axis=1,inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data=data[data['year']<2023]\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7152039",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train=preprocessing_client(client_train.copy())\n",
    "hist_weather_datetime_county,hist_weather_datetime=preprocessing_weather(hist_weather_train.copy(),locations)\n",
    "train=preprocessing_train(train.copy())\n",
    "electricity_train=preprocessing_electricity(electricity_train.copy())\n",
    "gas_price_train=preprocessing_gas(gas_price_train.copy())\n",
    "\n",
    "\n",
    "data=data_merge(train.copy(),client_train.copy(),hist_weather_datetime_county.copy(),hist_weather_datetime.copy(),electricity_train.copy(),gas_price_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c330ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() #Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f09ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97d2c2",
   "metadata": {},
   "source": [
    "**- Analysis..**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6f721",
   "metadata": {},
   "source": [
    "**-I will apply all strategies and answer all questions on (Group 1) . Behind the scense , I will implement the same things for remaining groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The answer to my questions : I want to split the data into a group of sections based on (prediction_unit_id)\n",
    "groups=data.groupby(['prediction_unit_id'])\n",
    "print(\"Number of types of product:\",len(groups.groups.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group 1\n",
    "group=groups.get_group(1)\n",
    "consumption=group[group['is_consumption']==1]\n",
    "production=group[group['is_consumption']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0162b3",
   "metadata": {},
   "source": [
    "**1)Consumption Analysis...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa274de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general information about product\n",
    "print(\"Number of counties in this product:\",len(consumption.county.unique()))\n",
    "answer=\"yes\" if consumption['is_business'].values[0] else \"no\"\n",
    "print(\"this product is business product?\",answer)\n",
    "product_type=consumption['product_type'].values[0]\n",
    "if product_type==0:\n",
    "  product_type=\"Combined\"\n",
    "elif product_type==1:\n",
    "  product_type='Fixed'\n",
    "elif product_type==2:\n",
    "  product_type=\"General service\"\n",
    "else:\n",
    "  product_type=\"Spot\"\n",
    "print(\"product type:\",product_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46274d2c",
   "metadata": {},
   "source": [
    "**- Q: When consumption highest for this product ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959850de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descrptive analysis for consumption\n",
    "display(consumption.target.describe())\n",
    "sns.distplot(consumption.target)\n",
    "plt.show()\n",
    "probplot(consumption.target, dist='norm', plot=plt)\n",
    "#result\n",
    "##The consumption of this product is clusterd around 20 ,\n",
    "##Change or distribution in its consumption is considered appropriate ,\n",
    "##I will search for reasons  any number > 34 or number < 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933fd0c",
   "metadata": {},
   "source": [
    "**- Q:What the appropriate budget to cover the requirements of each business product ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a103a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on budget you need in 2021\n",
    "#This is not a business product ,but anyway apply this: we calculated the total money from production during 2021 and subtracted the total money spent.\n",
    "consumption_21=consumption[consumption['year']==2021]\n",
    "production_21=production[production['year']==2021]\n",
    "energy_consumption = np.sum(consumption_21['target'].values)\n",
    "price_per_mwh = np.mean(consumption_21['euros_per_mwh'].values)\n",
    "total_cost_consumption = energy_consumption * price_per_mwh\n",
    "energy_production =np.sum( production_21['target'].values)\n",
    "lowest_price_per_mwh = np.mean(production_21['lowest_price_per_mwh'].values)\n",
    "highest_price_per_mwh = np.mean(production_21['highest_price_per_mwh'].values)\n",
    "average_price_per_mwh = (lowest_price_per_mwh + highest_price_per_mwh) / 2\n",
    "\n",
    "total_revenue_production = energy_production * average_price_per_mwh\n",
    "\n",
    "total_budget = total_revenue_production - total_cost_consumption\n",
    "print(total_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af9b44",
   "metadata": {},
   "source": [
    "**-Q: What are the most important reasons affecting production or consumption ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74525bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_inf_columns=consumption.drop(['county','is_business','product_type','prediction_unit_id','year','day','month','hour','is_consumption','datetime',\n",
    "                                       'lowest_price_per_mwh', 'highest_price_per_mwh', 'euros_per_mwh'],axis=1).columns\n",
    "possible_inf_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c160659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in features importanat.. \n",
    "# 'eic_count' --> The aggregated number of consumption points\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','eic_count','Target_2022','eic_count'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].eic_count),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].eic_count),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "#correlation\n",
    "sns.heatmap(consumption[['target','eic_count']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.eic_count.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear , p-value is very small(Nice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installed_capacity --> Installed photovoltaic solar panel capacity in kilowatts.\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','installed_capacity','Target_2022','installed_capacity'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].installed_capacity),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].installed_capacity),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','installed_capacity']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.installed_capacity.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is semi clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature_hist_mean --> the mean(based on datetime) forecast temperature\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','temperature_hist_mean','Target_2022','temperature_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].temperature_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].temperature_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','temperature_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.temperature_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is very clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature_hist_mean_by_county--> the mean(based on county) forecast temperature\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','temperature_hist_mean_county','Target_2022','temperature_hist_mean_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].temperature_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].temperature_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','temperature_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.temperature_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is very clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c21574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dewpoint_hist_mean--> the mean of dewpoint (based on datetime) forecast dewpoint\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','dewpoint_hist_mean','Target_2022','dewpoint_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].dewpoint_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].dewpoint_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','dewpoint_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.dewpoint_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is very clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dewpoint_hist_mean_by_county--> the mean of dewpoint (based on county) forecast dewpoint\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','dewpoint_hist_mean_by_county','Target_2022','dewpoint_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].dewpoint_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].dewpoint_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','dewpoint_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "##hypothesis testing\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.dewpoint_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is very clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_total_hist_mean--> the mean of cloudcover_total (based on datetime) forecast cloudcover_total\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_total_hist_mean','Target_2022','cloudcover_total_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_total_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_total_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_total_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_total_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_total_hist_mean_by_county--> the mean of cloudcover_total (based on county) forecast cloudcover_total\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_total_hist_mean_by_county','Target_2022','cloudcover_total_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_total_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_total_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_total_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_total_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear , and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80722d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_low_hist_mean--> the mean of cloudcover_low (based on datetime) forecast cloudcover_low\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_low_hist_mean','Target_2022','cloudcover_low_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_low_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_low_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_low_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_low_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_low_hist_mean_by_county--> the mean of cloudcover_low (based on county) forecast cloudcover_low\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_low_hist_mean_by_county','Target_2022','cloudcover_low_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_low_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_low_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_low_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_low_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14935ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_mid_hist_mean--> the mean of cloudcover_mid (based on datetime) forecast cloudcover_mid\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_mid_hist_mean','Target_2022','cloudcover_mid_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_mid_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_mid_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_mid_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_mid_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_mid_hist_mean_by_county--> the mean of cloudcover_mid (based on county) forecast cloudcover_mid\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_mid_hist_mean_by_county','Target_2022','cloudcover_mid_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_mid_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_mid_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_mid_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_mid_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_high_hist_mean--> the mean of cloudcover_high (based on datetime) forecast cloudcover_high\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_high_hist_mean','Target_2022','cloudcover_high_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_high_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_high_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_high_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_high_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value & correlation is very small --> small impact on target feature . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudcover_high_hist_mean_by_county--> the mean of cloudcover_high (based on county) forecast cloudcover_high\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','cloudcover_high_hist_mean_by_county','Target_2022','cloudcover_high_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].cloudcover_high_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].cloudcover_high_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','cloudcover_high_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.cloudcover_high_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is not clear (in visual), and p-value & correlation is very small --> small impact on target feature . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct_solar_radiation_hist_mean--> the mean of direct_solar_radiation_hist (based on datetime) forecast direct_solar_radiation\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','direct_solar_radiation_hist_mean','Target_2022','direct_solar_radiation_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].direct_solar_radiation_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].direct_solar_radiation_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','direct_solar_radiation_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.direct_solar_radiation_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is clear (in visual), and p-value is very small , coreelating is high (nice) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct_solar_radiation_hist_mean_by_county--> the mean of direct_solar_radiation_hist (based on county) forecast direct_solar_radiation\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','direct_solar_radiation_hist_mean_by_county','Target_2022','direct_solar_radiation_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].direct_solar_radiation_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].direct_solar_radiation_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','direct_solar_radiation_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.direct_solar_radiation_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is clear (in visual), and p-value is very small , coreelating is high (nice) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowfall_hist_mean--> the mean of snowfall_hist_mean (based on datetime) forecast snowfall\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','snowfall_hist_mean','Target_2022','snowfall_hist_mean'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].snowfall_hist_mean),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].snowfall_hist_mean),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','snowfall_hist_mean']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.snowfall_hist_mean.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is semi clear (in visual), and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowfall_hist_mean_by_county--> the mean of snowfall_hist_mean (based on county) forecast snowfall\n",
    "fig = sp.make_subplots(rows=2, cols=2, subplot_titles=['Target_2021','snowfall_hist_mean_by_county','Target_2022','snowfall_hist_mean_by_county'])\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].target),row=1,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2021].datetime, y=consumption[consumption['year']==2021].snowfall_hist_mean_by_county),row=1,col=2)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].target),row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=consumption[consumption['year']==2022].datetime, y=consumption[consumption['year']==2022].snowfall_hist_mean_by_county),row=2,col=2)\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n",
    "sns.heatmap(consumption[['target','snowfall_hist_mean_by_county']].corr(),\n",
    "            annot=True,\n",
    "            cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "t_statistic, p_value = ttest_ind(consumption.target.values, consumption.snowfall_hist_mean_by_county.values)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n",
    "\n",
    "#results\n",
    "##The relationship is semi clear (in visual), and p-value is equal 0 this means:\n",
    "       # 1)There is strong evidence that the observed effect is real and not due to random chance.\n",
    "       # 2) A p-value of 0.0 is below any conventional significance level (e.g., 0.05 or 0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7260f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to apply a specific experiment to test our analysis..\n",
    "\n",
    "#build a ML on choosed features\n",
    "x=consumption[['installed_capacity','temperature_hist_mean','dewpoint_hist_mean','cloudcover_total_hist_mean',\n",
    "               'cloudcover_low_hist_mean','direct_solar_radiation_hist_mean','snowfall_hist_mean','year','hour','day']]\n",
    "y=consumption['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=4, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "##build a ML on all features\n",
    "x=consumption.drop(['target','datetime','prediction_unit_id'],axis=1)\n",
    "y=consumption['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=4, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error(all): {mse}\")\n",
    "print(f\"R-squared(all): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list=['eic_count','cloudcover_mid_hist_mean','loudcover_mid_hist_mean_by_county','cloudcover_high_hist_mean','cloudcover_high_hist_mean_by_county',\n",
    "           'temperature_hist_mean_by_county',\n",
    "       'dewpoint_hist_mean_by_county', 'cloudcover_total_hist_mean_by_county',\n",
    "       'cloudcover_low_hist_mean_by_county',\n",
    "       'cloudcover_mid_hist_mean_by_county',\n",
    "       'direct_solar_radiation_hist_mean_by_county',\n",
    "       'snowfall_hist_mean_by_county']\n",
    "mask = np.isin(possible_inf_columns, drop_list, invert=True)\n",
    "possible_inf_columns=possible_inf_columns[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e1bdd",
   "metadata": {},
   "source": [
    "**- Answer Based on above:**\n",
    "\n",
    "  **(target,installed_capacity,temperature_hist_mean,dewpoint_hist_mean,cloudcover_total_hist_mean,\n",
    "      ,cloudcover_low_hist_mean,direct_solar_radiation_hist_mean,snowfall_hist_mean) and of course datetime(month,year,day,hour).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95752e4",
   "metadata": {},
   "source": [
    "**- Q: What are the perfect conditions for each product in terms of the most important factors affecting it?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to find the hours during which the product makes profits abd extract from them the appropriate conditions for it \n",
    "energy_consumption = consumption['target'].values\n",
    "price_per_mwh =consumption['euros_per_mwh'].values\n",
    "total_cost_consumption = energy_consumption * price_per_mwh\n",
    "energy_production =production['target'].values\n",
    "lowest_price_per_mwh = production['lowest_price_per_mwh'].values\n",
    "highest_price_per_mwh = production['highest_price_per_mwh'].values\n",
    "average_price_per_mwh = (lowest_price_per_mwh + highest_price_per_mwh) / 2\n",
    "total_revenue_production = energy_production * average_price_per_mwh\n",
    "total_budget = total_revenue_production - total_cost_consumption\n",
    "profit_data=pd.concat([consumption[total_budget>0],production[total_budget>0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(rows=8, cols=1, subplot_titles=['target','installed_capacity',\n",
    " 'temperature_hist_mean','dewpoint_hist_mean','cloudcover_total_hist_mean','cloudcover_low_hist_mean','direct_solar_radiation_hist_mean',\n",
    " 'snowfall_hist_mean'])\n",
    "start=0\n",
    "end=0\n",
    "profit_data=profit_data.sort_values(by='datetime')\n",
    "for row in range(8):\n",
    "    fig.add_trace(go.Scatter(x=profit_data.datetime, y=profit_data[f'{possible_inf_columns[row]}']),row=row+1,col=1)\n",
    "  \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "                 height=2000,\n",
    "                 template='plotly_dark',\n",
    "                  )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d24a2",
   "metadata": {},
   "source": [
    "**-⁠ Q:When is product consumption above the limit or outlier?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will apply two methods for the outlier\n",
    "#Outlier univariate\n",
    "#The goal of the first method is to achieve  significantly higher consumption\n",
    "data=consumption.target.values\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Set your Z-score threshold (commonly 2 or 3)\n",
    "z_threshold = 2.5\n",
    "\n",
    "# Perform univariate outlier detection\n",
    "z_scores = np.abs(stats.zscore(normalized_data))\n",
    "outliers = data[z_scores > z_threshold]\n",
    "non_outliers = data[z_scores <= z_threshold]\n",
    "\n",
    "# Print or use the results as needed\n",
    "print(\"Detected outliers:\", outliers.shape[0]/consumption.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_targets=consumption.iloc[z_scores > z_threshold]\n",
    "drop_list=['eic_count','cloudcover_mid_hist_mean','loudcover_mid_hist_mean_by_county','cloudcover_high_hist_mean','cloudcover_high_hist_mean_by_county',\n",
    "           'temperature_hist_mean_by_county',\n",
    "       'dewpoint_hist_mean_by_county', 'cloudcover_total_hist_mean_by_county',\n",
    "       'cloudcover_low_hist_mean_by_county',\n",
    "       'cloudcover_mid_hist_mean_by_county',\n",
    "       'direct_solar_radiation_hist_mean_by_county',\n",
    "       'snowfall_hist_mean_by_county']\n",
    "mask = np.isin(possible_inf_columns, drop_list, invert=True)\n",
    "possible_inf_columns=possible_inf_columns[mask]\n",
    "fig = sp.make_subplots(rows=8, cols=1, subplot_titles=['target','installed_capacity',\n",
    " 'temperature_hist_mean','dewpoint_hist_mean','cloudcover_total_hist_mean','cloudcover_low_hist_mean',\n",
    "'direct_solar_radiation_hist_mean','snowfall_hist_mean'])\n",
    "start=0\n",
    "end=0\n",
    "max_targets=max_targets.sort_values(by='datetime')\n",
    "for row in range(8):\n",
    "    fig.add_trace(go.Scatter(x=max_targets.datetime, y=max_targets[f'{possible_inf_columns[row]}']),row=row+1,col=1)\n",
    "  \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "                 height=2000,\n",
    "                 template='plotly_dark',\n",
    "                  )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier\n",
    "#our goal on extarcting outliers is to identify unexpected behavior from the product \n",
    "lof = LocalOutlierFactor(n_neighbors=5)\n",
    "outlier_labels = lof.fit_predict(consumption[possible_inf_columns])\n",
    "print('percentage:',consumption[outlier_labels==-1].shape[0]/consumption.shape[0])\n",
    "#if percentage is large this is an indeication that there is something wrong in the product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on production's product \n",
    "diff=production.target.values-consumption.target.values\n",
    "print(\"number of negative hours production:\",consumption[diff<=0].shape[0])\n",
    "print(\"total number of hours production:\",consumption.shape[0])\n",
    "print(\"percentage of negative hours production:\",consumption[diff<=0].shape[0]/consumption.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37271fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad_conditions\n",
    "bad_conditions=consumption[diff<=0]\n",
    "fig = sp.make_subplots(rows=8, cols=1, subplot_titles=['target','installed_capacity',\n",
    " 'temperature_hist_mean','dewpoint_hist_mean','cloudcover_total_hist_mean','cloudcover_low_hist_mean',\n",
    "'direct_solar_radiation_hist_mean','snowfall_hist_mean'])\n",
    "start=0\n",
    "end=0\n",
    "bad_conditions=bad_conditions.sort_values(by='datetime')\n",
    "for row in range(8):\n",
    "    fig.add_trace(go.Scatter(x=bad_conditions.datetime, y=bad_conditions[f'{possible_inf_columns[row]}']),row=row+1,col=1)\n",
    "  \n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "                 height=2000,\n",
    "                 template='plotly_dark',\n",
    "                  )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a00611",
   "metadata": {},
   "source": [
    "**-Reapte apply this on all consumption and production product .**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a117be9",
   "metadata": {},
   "source": [
    "# - Construct.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906dd4a",
   "metadata": {},
   "source": [
    "**- Q: What is the needed transformation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we are dealing with TimeSieres data , I am supposed to provides the model with the value of my targe before n days\n",
    "def create_n_day_lags(data, N_day_lags):\n",
    "    original_datetime = data['datetime']\n",
    "    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n",
    "    for day_lag in range(2, N_day_lags+1):\n",
    "        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n",
    "        data = data.merge(revealed_targets, \n",
    "                          how='left', on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n",
    "                          suffixes = ('', f'_{day_lag}_days_ago'))\n",
    "    data['sin_hour']= (np.pi * np.sin(data['hour']) / 12)\n",
    "    data['cos_hour']= (np.pi * np.cos(data['hour']) / 12)\n",
    "    data['target_mean']= data[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].mean(1)\n",
    "    data['target_std']= data[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].std(1)\n",
    "    data['target_var']= data[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].var(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_skew_data(data):\n",
    "    skew_df = pd.DataFrame(data.select_dtypes(np.number).columns, columns=['Feature'])\n",
    "    skew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(data[feature]))\n",
    "    skew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\n",
    "    skew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\n",
    "    skew_df=skew_df[~skew_df['Feature'].isin(['year', 'prediction_unit_id'])]\n",
    "    return ( skew_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d005d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Editing the distribution for some columns that have a significant deviation\n",
    "present_skew_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa896c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_data(data):\n",
    "    skew_df = pd.DataFrame(data.select_dtypes(np.number).columns, columns=['Feature'])\n",
    "    skew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(data[feature]))\n",
    "    skew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\n",
    "    skew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\n",
    "    skew_df=skew_df[~skew_df['Feature'].isin(['year', 'prediction_unit_id'])]\n",
    "    columns=skew_df[skew_df['Skewed']==True].Feature.values\n",
    "    data=distribution_preprocessing(data.copy(),columns)\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_preprocessing(data,columns):\n",
    "    for i in columns:\n",
    "        data[f\"{i}\"]= np.where((data[i])!= 0, np.log(data[i]),0)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a45e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply functions\n",
    "data=create_n_day_lags(data.copy(),7)\n",
    "data=skew_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneeded columns\n",
    "data.drop(['datetime','prediction_unit_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e0185",
   "metadata": {},
   "source": [
    "**-Q: Do we need tow models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3338b6d",
   "metadata": {},
   "source": [
    "- Yes , we need to build two models , one for consumption and other for production \n",
    "- but , why?Because of the different goal for each model , it is illogical to build a single model for two completely contraductory goal , instead od the definitely different patterns between them and the influencing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d33cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_is_consumption= data[data['is_consumption'] != 0].drop('target', axis= 1)\n",
    "y_is_consumption= data[data['is_consumption'] != 0]['target']\n",
    "x_production= data[data['is_consumption'] == 0].drop('target', axis= 1)\n",
    "y_production= data[data['is_consumption'] == 0]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21987c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_is_consumption_train,x_is_consumption_test,y_is_consumption_train,y_is_consumption_test=train_test_split(x_is_consumption,y_is_consumption,test_size=0.3,shuffle=True, random_state=5)\n",
    "x_production_train,x_production_test,y_production_train,y_production_test=train_test_split(x_production,y_production,test_size=0.3, shuffle=True, random_state=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d13a9e",
   "metadata": {},
   "source": [
    "**-Q:Which is the best model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9f281",
   "metadata": {},
   "source": [
    "- We have experienced a group of algo , the best of which , as a result , is RandomForest.\n",
    "- An important note, the resources available to us limit our use of some complext algorithms . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ab1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply gridsearch for optemize parameters..\n",
    "#xgb_is_consumption = xgb.XGBRegressor()\n",
    "#param_grid = {\n",
    "   # 'n_estimators': [4,100, 200, 300],  \n",
    "   # 'max_depth': [3, 4, 5],  \n",
    "   # 'learning_rate': [0.1, 0.01, 0.001]  \n",
    "#}\n",
    "\n",
    "#grid_search = GridSearchCV(estimator=xgb_is_consumption , param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
    "#grid_result = grid_search.fit(x_is_consumption_train,y_is_consumption_train)\n",
    "\n",
    "#print(\"Best Parameters:\", grid_result.best_params_)\n",
    "#print(\"Best Score:\", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_is_consumption = RandomForestRegressor(n_estimators=4)\n",
    "xgb_is_consumption.fit(x_is_consumption_train,y_is_consumption_train)\n",
    "y_is_consumption_pred=xgb_is_consumption.predict(x_is_consumption_test)\n",
    "mse = mean_squared_error(y_is_consumption_test, y_is_consumption_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_is_consumption_test, y_is_consumption_pred)\n",
    "r_squared = r2_score(y_is_consumption_test, y_is_consumption_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r_squared}\")\n",
    "x_is_consumption_test['log_traget_predicting']=y_is_consumption_pred\n",
    "x_is_consumption_test['target']=y_is_consumption_test\n",
    "x_is_consumption_test.loc[x_is_consumption_test['log_traget_predicting']<0,'log_traget_predicting']=0\n",
    "x_is_consumption_test['traget_predicting']=np.exp(y_is_consumption_pred)\n",
    "x_is_consumption_test['diff']=x_is_consumption_test['log_traget_predicting']-x_is_consumption_test['target']\n",
    "x_is_consumption_test[['traget_predicting','log_traget_predicting','target','diff']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b70b6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation \n",
    "#scores = cross_val_score(RandomForestRegressor(n_estimators=4),x_is_consumption,y_is_consumption, cv=5)\n",
    "#print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_production = RandomForestRegressor(n_estimators=4)\n",
    "xgb_production.fit(x_production_train,y_production_train)\n",
    "y_production_pred=xgb_production.predict(x_production_test)\n",
    "mse = mean_squared_error(y_production_test, y_production_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_production_test, y_production_pred)\n",
    "r_squared = r2_score(y_production_test, y_production_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r_squared}\")\n",
    "x_production_test['log_traget_predicting']=y_production_pred\n",
    "x_production_test['traget_predicting']=np.exp(y_production_pred)\n",
    "x_production_test['target']=y_production_test\n",
    "x_production_test.loc[x_production_test['log_traget_predicting']<0,'log_traget_predicting']=0\n",
    "x_production_test['traget_predicting']=np.exp(y_production_pred)\n",
    "x_production_test['diff']=x_production_test['log_traget_predicting']-x_production_test['target']\n",
    "x_production_test[['traget_predicting','log_traget_predicting','target','diff']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation \n",
    "scores = cross_val_score(RandomForestRegressor(n_estimators=4),x_is_production,y_is_production, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addcbe0",
   "metadata": {},
   "source": [
    "**- A questions may come to your mind : Why did use only  the 4 estimators? we did'nt see much difference in accuracy !! , Indeed, if this accuracy is achieved with least complexity of the model structure, this is perfect ..**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c35d7c",
   "metadata": {},
   "source": [
    "# - Execute.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c0052",
   "metadata": {},
   "source": [
    "**- Some useful tips:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55cb19",
   "metadata": {},
   "source": [
    "**- There are some products with high energy consumption in some special circumstances, there are tow solutions to this as for dispensing with theses prosucts in special circumstances reduce the working time as possible .**\n",
    "\n",
    "**- Noticed that there are somw weak point in each product that is written in the analysis stage , we must work on solving these weak points.**\n",
    "\n",
    "**- There are some product that work at certain times and some cases due to circumstances.Is it possible to change the time? In order to reduce energy consumption.**\n",
    "\n",
    "**- We must pay attention to the difference between the product's energy production and its energy consumption during half the day, why? Because if it is negative , we must know that there is a problem and it is most likely caused by weather conditions .**\n",
    "\n",
    "**- Noticed a climate pattern for each county at the same time for each year. \n",
    "we don't know the nature of the locations of the products , but if their locations can be changed , the there are counties that are not suitable for some products due to their weather conditions .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029c7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
